{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle physics results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "n_chains = 4\n",
    "n_trueparams = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_filenames = []\n",
    "algo_additionals = []\n",
    "algo_labels = []\n",
    "algo_dividers = []\n",
    "algo_dims = []\n",
    "\n",
    "def add_algo(filename, add, label, dim=\"\"):\n",
    "    algo_filenames.append(filename)\n",
    "    algo_additionals.append(add)\n",
    "    algo_labels.append(label)\n",
    "    algo_dims.append(dim)\n",
    "    \n",
    "    \n",
    "def add_divider():\n",
    "    algo_dividers.append(len(algo_filenames))\n",
    "\n",
    "add_algo(\"flow\", \"_june\", r\"\\af{}\", \"40d\")\n",
    "add_algo(\"pie\", \"_conditionalmanifold_june\", r\"\\pie{} (original)\", \"40d\")\n",
    "add_algo(\"pie\", \"_june\", r\"\\pie{} (unconditional manifold)\", \"40d\")\n",
    "add_algo(\"pae\", \"_june\", r\"\\pae{}\", \"40d\")\n",
    "add_algo(\"mf\", \"_june\", r\"\\mf{}\", \"40d\")\n",
    "add_algo(\"emf\", \"_june\", r\"\\mfe{}\", \"40d\")\n",
    "\n",
    "add_divider()\n",
    "\n",
    "add_algo(\"flow\", \"_scandal_june\", r\"\\af{} (\\scandal{})\", \"40d\")\n",
    "add_algo(\"pie\", \"_conditionalmanifold_scandal_june\", r\"\\pie{} (original, \\scandal{})\", \"40d\")\n",
    "add_algo(\"pie\", \"_scandal_june\", r\"\\pie{} (uncond.~manifold, \\scandal{})\", \"40d\")\n",
    "add_algo(\"pae\", \"_scandal_june\", r\"\\pae{} (\\scandal{})\", \"40d\")\n",
    "add_algo(\"mf\", \"_scandal_june\", r\"\\mf{} (\\scandal{})\", \"40d\")\n",
    "add_algo(\"emf\", \"_scandal_june\", r\"\\mfe{} (\\scandal{})\", \"40d\")\n",
    "\n",
    "add_divider()\n",
    "\n",
    "add_algo(\"alices\", \"_may\", r\"Likelihood ratio estimator (\\alices{})\")\n",
    "\n",
    "n_algos = len(algo_filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(name, shape, numpyfy=True, chains=1, result_dir=\"../data/results\"):\n",
    "    all_results = []\n",
    "    \n",
    "    for algo_filename, algo_add, algo_dim in zip(algo_filenames, algo_additionals, algo_dims):\n",
    "        algo_results = []\n",
    "            \n",
    "        for run in range(n_runs):\n",
    "            run_str = \"\" if run == 0 else \"_run{}\".format(run)\n",
    "            \n",
    "            for trueparam in range(n_trueparams):\n",
    "                trueparam_str = \"\" if trueparam == 0 else \"_trueparam{}\".format(trueparam)\n",
    "\n",
    "                try:\n",
    "                    this_result = np.load(\n",
    "                        \"{}/{}_{}_lhc{}{}{}_{}{}.npy\".format(\n",
    "                            result_dir, algo_filename, \"2\" if algo_dim == \"2d\" else \"14\",\n",
    "                            algo_dim, algo_add, run_str, name, trueparam_str\n",
    "                        )\n",
    "                    )\n",
    "                    if (not numpyfy) or (shape is None) or np.product(this_result.shape) == np.product(shape):\n",
    "                        algo_results.append(this_result.reshape(shape))\n",
    "                    else:\n",
    "                        algo_results.append(np.nan*np.ones(shape))\n",
    "\n",
    "                except FileNotFoundError as e:\n",
    "                    # print(e)\n",
    "                    if shape is None:\n",
    "                        algo_results.append(None)\n",
    "                    else:\n",
    "                        algo_results.append(np.nan*np.ones(shape))\n",
    "                except ValueError as e:\n",
    "                    print(e)\n",
    "                    if shape is None:\n",
    "                        algo_results.append(None)\n",
    "                    else:\n",
    "                        algo_results.append(np.nan*np.ones(shape))\n",
    "            \n",
    "        all_results.append(algo_results)\n",
    "    \n",
    "    if numpyfy:\n",
    "        all_results = np.array(all_results, dtype=np.float)\n",
    "        all_results = all_results.reshape([all_results.shape[0], n_runs, n_trueparams] + list(shape))\n",
    "        \n",
    "    return all_results\n",
    "\n",
    "\n",
    "model_gen_x = load(\"samples\", None, numpyfy=False)\n",
    "model_gen_closure = load(\"samples_manifold_distance\", (10000,))\n",
    "model_test_reco_error = load(\"model_reco_error_test\", (1000,))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mcmc(name, shape, numpyfy=True, result_dir=\"../data/results\"):\n",
    "    all_results = []\n",
    "    \n",
    "    for algo_filename, algo_add, algo_dim in zip(algo_filenames, algo_additionals, algo_dims):\n",
    "        algo_results = []\n",
    "            \n",
    "        for run in range(n_runs):\n",
    "            run_str = \"\" if run == 0 else \"_run{}\".format(run)\n",
    "            \n",
    "            for trueparam in range(n_trueparams):\n",
    "                trueparam_str = \"\" if trueparam == 0 else \"_trueparam{}\".format(trueparam)\n",
    "            \n",
    "                for chain in range(n_chains):\n",
    "                    chain_str = \"\" if chain == 0 else \"_chain{}\".format(chain)\n",
    "\n",
    "                    try:\n",
    "                        this_result = np.load(\n",
    "                            \"{}/{}_{}_lhc{}{}{}_{}{}{}.npy\".format(\n",
    "                                result_dir, algo_filename, \"2\" if algo_dim == \"2d\" else \"14\",\n",
    "                                algo_dim, algo_add, run_str, name, trueparam_str, chain_str\n",
    "                            )\n",
    "                        )\n",
    "                        if (not numpyfy) or (shape is None) or np.product(this_result.shape) == np.product(shape):\n",
    "                            algo_results.append(this_result.reshape(shape))\n",
    "                        else:\n",
    "                            algo_results.append(np.nan*np.ones(shape))\n",
    "\n",
    "                    except FileNotFoundError as e:\n",
    "                        # print(e)\n",
    "                        if shape is None:\n",
    "                            algo_results.append(None)\n",
    "                        else:\n",
    "                            algo_results.append(np.nan*np.ones(shape))\n",
    "            \n",
    "        all_results.append(algo_results)\n",
    "    \n",
    "    all_results = np.array(all_results, dtype=np.float)\n",
    "    all_results = all_results.reshape([all_results.shape[0], n_runs, n_trueparams, n_chains] + list(shape))\n",
    "        \n",
    "    return all_results\n",
    "\n",
    "\n",
    "model_posterior_samples = load_mcmc(\"posterior_samples\", (2500, 2,))\n",
    "model_posterior_samples.shape  # (algo, run, true param id, chain, sample, theta component)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gen_mean_closure = np.mean(model_gen_closure, axis=(2,3))\n",
    "model_gen_mean_closure.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_reco_error = 10.\n",
    "model_mean_reco_error = np.mean(np.clip(model_test_reco_error, 0., max_reco_error), axis=(2,3))\n",
    "model_mean_reco_error.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidth = 0.15\n",
    "true_param_points = np.array([[0.,0.], [0.5, 0.], [-1., -1.]])\n",
    "\n",
    "model_true_log_posteriors = []\n",
    "\n",
    "for algo, run, trueparam in product(range(n_algos), range(n_runs), range(n_trueparams)):\n",
    "    mcmcs = model_posterior_samples[algo, run, trueparam].reshape((-1, 2))\n",
    "    mcmcs = mcmcs[np.all(np.isfinite(mcmcs), axis=-1)]\n",
    "    \n",
    "    if len(mcmcs) == 0:\n",
    "        model_true_log_posteriors.append(np.nan)\n",
    "        continue\n",
    "        \n",
    "    kde = KernelDensity(kernel=\"gaussian\", bandwidth=bandwidth)\n",
    "    kde.fit(mcmcs)\n",
    "    model_true_log_posteriors.append(kde.score(true_param_points[trueparam].reshape((1, 2))))\n",
    "\n",
    "model_true_log_posteriors = np.mean(np.array(model_true_log_posteriors).reshape((n_algos, n_runs, n_trueparams)), axis=-1)\n",
    "model_true_log_posteriors.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_err_without_outliers(data, remove=1):\n",
    "    shape = list(data.shape)[:-1]\n",
    "    data.reshape((-1, data.shape[-1]))\n",
    "    \n",
    "    means, errors = [], []\n",
    "    \n",
    "    for data_ in data:\n",
    "        data_ = data_[np.isfinite(data_)]\n",
    "        if not len(data_) > 0:\n",
    "            means.append(np.nan)\n",
    "            errors.append(np.nan)\n",
    "            continue\n",
    "            \n",
    "        if len(data_) > 2*remove + 1:\n",
    "            for _ in range(remove):\n",
    "                data_ = np.delete(data_, np.argmin(data_))\n",
    "                data_ = np.delete(data_, np.argmax(data_))\n",
    "\n",
    "        means.append(np.mean(data_))\n",
    "        errors.append(np.std(data_) / len(data_)**0.5)\n",
    "        \n",
    "    return np.array(means).reshape(shape), np.array(errors).reshape(shape)\n",
    "    \n",
    "    \n",
    "model_true_log_posteriors_mean, model_true_log_posteriors_std = mean_err_without_outliers(model_true_log_posteriors)\n",
    "model_gen_mean_closure_mean, model_gen_mean_closure_std = mean_err_without_outliers(model_gen_mean_closure)\n",
    "model_mean_reco_error_mean, model_mean_reco_error_std = mean_err_without_outliers(model_mean_reco_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_closure, best_posterior = -1, -1\n",
    "\n",
    "best_closure = np.nanargmin(model_gen_mean_closure_mean)\n",
    "print(algo_labels[best_closure])\n",
    "\n",
    "best_reco = np.nanargmin(np.where(model_mean_reco_error_mean > 1.e-3, model_mean_reco_error_mean, np.nan))\n",
    "print(algo_labels[best_reco])\n",
    "    \n",
    "best_posterior = np.nanargmax(model_true_log_posteriors_mean)\n",
    "print(algo_labels[best_posterior])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print result table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(\n",
    "    l_label=max([len(l) for l in algo_labels]), l_means=(6, 5, 5), l_errs=(6, 5, 4), latex=False, after_decs=(4,3,2)\n",
    "):\n",
    "    # Number of digits\n",
    "    l_results = np.array(l_means) + 2 + np.array(l_errs)\n",
    "    l_total = l_label + 1 + np.sum(3 + l_results)\n",
    "        \n",
    "    # Divider\n",
    "    col_divider = \"&\" if latex else \"|\"\n",
    "    line_end = r\"\\\\\" if latex else \"\"\n",
    "    block_divider = r\"\\midrule\" if latex else \"-\"*l_total\n",
    "    \n",
    "    # Number formatting\n",
    "    def _f(val, err, after_dec, l_mean, l_err, best=False):\n",
    "        l_result = l_mean + 2 + l_err\n",
    "        empty_result = \"\" if latex else \" \"*(l_result + 1)\n",
    "        \n",
    "        if not np.any(np.isfinite(val)):\n",
    "            return empty_result\n",
    "        \n",
    "        result = \"{:>{}.{}f}\".format(val, l_mean, after_dec)\n",
    "        if latex and best:\n",
    "            result = r\"\\textbf{\" + result + \"}\"\n",
    "            \n",
    "        if latex:\n",
    "            err_str = str.rjust(\"{:.{}f}\".format(err, after_dec), l_err).replace(\" \", r\"\\hphantom{0}\")\n",
    "            result += r\"\\;\\textcolor{darkgray}{$\\pm$\\;\" + err_str + \"}\"\n",
    "        else:\n",
    "            err_str = \"({:>{}.{}f})\".format(err, l_err, after_dec)\n",
    "            result += err_str\n",
    "            \n",
    "        result += \"*\" if not latex and best else \" \"\n",
    "        \n",
    "        if latex:\n",
    "            result = result.replace(\"-\", \"$-{}$\")\n",
    "            result = result.replace(\"darkgray\", \"dark-gray\")\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    # Header\n",
    "    print(\n",
    "        \"{2:<{0}.{0}s} {5} {3:>{1}.{1}s} {5} {7:>{8}.{8}s} {5} {4:>{9}.{9}s} {6}\".format(\n",
    "            l_label, l_results[0], \"\", \"Closure\", \"log p\", col_divider, line_end, \"Reco error\", l_results[1], l_results[2]\n",
    "        )\n",
    "    )\n",
    "    print(block_divider)\n",
    "\n",
    "    # Iterate over methods\n",
    "    for i, (label, closure, closure_err, posterior, posterior_err, reco, reco_err) in enumerate(zip(\n",
    "        algo_labels,\n",
    "        model_gen_mean_closure_mean,\n",
    "        model_gen_mean_closure_std,\n",
    "        model_true_log_posteriors_mean,\n",
    "        model_true_log_posteriors_std,\n",
    "        model_mean_reco_error_mean,\n",
    "        model_mean_reco_error_std,\n",
    "    )):\n",
    "        # Divider\n",
    "        if i in algo_dividers:\n",
    "            print(block_divider)\n",
    "            \n",
    "        # Print results\n",
    "        print(\n",
    "            \"{1:<{0}.{0}s} {4} {2}{4} {6}{4} {3} {5}\".format(\n",
    "                l_label, label,\n",
    "                _f(closure, closure_err, after_decs[0], l_means[0], l_errs[0], i==best_closure),\n",
    "                _f(posterior, posterior_err, after_decs[2], l_means[2], l_errs[2], i==best_posterior),\n",
    "                col_divider, line_end,\n",
    "                _f(reco, reco_err, after_decs[1], l_means[1], l_errs[1], i==best_reco),\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(latex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual run results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_label=max([len(l) for l in algo_labels])\n",
    "l_mean=7\n",
    "after_decs=3\n",
    "\n",
    "# How to format the numbers\n",
    "l_result = 3 + n_runs*l_mean + (n_runs - 1)*2\n",
    "l_total = l_label + 4 + l_result\n",
    "\n",
    "# Divider\n",
    "empty_result = \" \"*(l_result + 1)\n",
    "col_divider = \"|\"\n",
    "line_end = \"\"\n",
    "block_divider = \"-\"*l_total\n",
    "    \n",
    "def _f(val, after_dec, best=False):\n",
    "    if not np.any(np.isfinite(val)):\n",
    "        return empty_result\n",
    "    result = \" [{:>{}.{}f}, \".format(np.nanmean(val[0]), l_mean, after_dec)\n",
    "    for i in range(1, n_runs - 1):\n",
    "        result += \"{:>{}.{}f}, \".format(np.nanmean(val[i]), l_mean, after_dec)\n",
    "    result += \"{:>{}.{}f}]\".format(np.nanmean(val[-1]), l_mean, after_dec)\n",
    "    result = result.replace(\"nan\", \"   \")\n",
    "    result += \"*\" if best else \" \"\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "# Print closure results\n",
    "print(\n",
    "    \"{2:<{0}.{0}s} {4} {3:>{1}.{1}s} {5}\".format(\n",
    "        l_label, l_result, \"\", \"Closure\", col_divider, line_end\n",
    "    )\n",
    ")\n",
    "print(block_divider)\n",
    "\n",
    "for i, (label, closure) in enumerate(zip(algo_labels, model_gen_mean_closure)):\n",
    "    # Divider\n",
    "    if i in algo_dividers:\n",
    "        print(block_divider)\n",
    "            \n",
    "    # Print results\n",
    "    print(\"{1:<{0}.{0}s} {3} {2} {4}\".format(\n",
    "        l_label, label, _f(closure, after_decs, i==best_closure), col_divider, line_end\n",
    "    ))\n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "    \n",
    "# Print reco error results\n",
    "print(\n",
    "    \"{2:<{0}.{0}s} {4} {3:>{1}.{1}s} {5}\".format(\n",
    "        l_label, l_result, \"\", \"Reco error\", col_divider, line_end\n",
    "    )\n",
    ")\n",
    "print(block_divider)\n",
    "\n",
    "for i, (label, reco) in enumerate(zip(algo_labels, model_test_reco_error)):\n",
    "    # Divider\n",
    "    if i in algo_dividers:\n",
    "        print(block_divider)\n",
    "            \n",
    "    # Print results\n",
    "    print(\"{1:<{0}.{0}s} {3} {2} {4}\".format(\n",
    "        l_label, label, _f(reco, after_decs, i==best_reco), col_divider, line_end\n",
    "    ))\n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "    \n",
    "# Print posterior results\n",
    "print(\n",
    "    \"{2:<{0}.{0}s} {4} {3:>{1}.{1}s} {5}\".format(\n",
    "        l_label, l_result, \"\", \"Log posterior\", col_divider, line_end\n",
    "    )\n",
    ")\n",
    "print(block_divider)\n",
    "\n",
    "for i, (label, posterior) in enumerate(zip(algo_labels, model_true_log_posteriors)):\n",
    "    # Divider\n",
    "    if i in algo_dividers:\n",
    "        print(block_divider)\n",
    "            \n",
    "    # Print results\n",
    "    print(\"{1:<{0}.{0}s} {3} {2} {4}\".format(\n",
    "        l_label, label, _f(posterior, after_decs, i==best_posterior), col_divider, line_end\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting the reco error in perspective: what if everything was just bleak randomness?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(size=(1000, 48))\n",
    "y = np.random.normal(size=(1000, 48))\n",
    "\n",
    "np.mean(np.sum((x - y) ** 2, axis=1) ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
